# Python Inference Service Configuration - OPTIMIZED FOR SPEED ON M2

# Server Configuration
INFERENCE_HOST=127.0.0.1
INFERENCE_PORT=9001
RELOAD=false
LOG_LEVEL=info

# Model Paths (relative to project root)
LLM_MODEL_PATH=models/MN-Violet-Lotus-12B-Q4_K_M.gguf
EMOTION_MODEL_PATH=models/roberta_emotions_onnx

# LLM Performance Settings - OPTIMIZED FOR MAXIMUM SPEED ON M2
# Metal acceleration is ENABLED (confirmed by ggml_metal_init messages)

# GPU Acceleration - Metal on M2
LLM_N_GPU_LAYERS=-1    # All layers on GPU (fastest)

# Context size - REDUCED for faster inference
# 10000 is TOO HIGH and causes slowdown
# 4096 is the sweet spot for speed + enough conversation memory
LLM_N_CTX=4096

# Batch size - OPTIMIZED for M2 speed
# 1024 is too high, causes slower processing
# 512 is optimal balance
LLM_N_BATCH=512

# CPU Threads - OPTIMAL for M2
# 6 threads prevents thermal throttling while maintaining speed
# 8 threads can cause the M2 to throttle under sustained load
LLM_N_THREADS=6

# CORS Configuration
ALLOWED_ORIGINS=http://localhost:9000,http://127.0.0.1:9000,https://localhost:9000,https://127.0.0.1:9000

# EXPECTED PERFORMANCE WITH THESE SETTINGS:
# - Response time: 5-15 seconds (down from 60 seconds)
# - Token generation: 15-25 tokens/sec
# - Memory usage: ~8-10GB
